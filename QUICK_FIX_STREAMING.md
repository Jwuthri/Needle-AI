# Quick Fix for Streaming Issues

## ‚úÖ Backend Fixed
- Fixed `AttributeError: TOOL_USE` ‚Üí using correct enum `LLMCallTypeEnum.SYSTEM`
- Fixed `AttributeError: 'LLMLogger' object has no attribute 'log_llm_call'` ‚Üí using `LLMCallRepository.create()` directly
- All LLM calls now logging correctly to database

## ‚ö†Ô∏è Frontend Needs Manual Update

The frontend chat-view.tsx needs to be updated to use streaming. Here's how:

### Step 1: Update chat-view.tsx imports (ALREADY DONE)
```typescript
import { useChatStream } from '@/hooks/use-chat-stream'
import { Loader } from 'lucide-react'
```

### Step 2: Add streaming hook after line 24 (ALREADY DONE)
```typescript
// Streaming chat hook
const { 
  sendMessage: sendStreamMessage, 
  isStreaming, 
  currentContent, 
  currentTree, 
  status: streamStatus 
} = useChatStream({
  onComplete: (response) => {
    const newMessage: EnhancedChatMessage = {
      id: response.message_id,
      content: response.message,
      role: 'assistant',
      timestamp: response.timestamp,
      metadata: response.metadata,
      execution_tree: response.execution_tree,
      visualization: response.visualization,
      sources: response.sources,
      output_format: response.output_format
    }
    setMessages(prev => [...prev, newMessage])
    setIsLoading(false)
  },
  onError: (error) => {
    console.error('Streaming error:', error)
    setIsLoading(false)
  }
})
```

### Step 3: Update handleSendMessage function (around line 94)

**REPLACE THE ENTIRE `handleSendMessage` FUNCTION WITH:**

```typescript
const handleSendMessage = async (message: string) => {
  if (!message.trim()) return

  setIsLoading(true)

  // Add user message immediately
  const userMessage: EnhancedChatMessage = {
    id: Date.now().toString(),
    content: message,
    role: 'user',
    timestamp: new Date().toISOString()
  }
  setMessages(prev => [...prev, userMessage])

  try {
    const token = await getToken()
    const api = createApiClient(token)

    // Create a new session if one doesn't exist
    let currentSessionId = sessionId
    if (!currentSessionId) {
      const newSession = await api.createSession()
      currentSessionId = newSession.session_id
      if (onSessionIdChange) {
        onSessionIdChange(currentSessionId)
      }
    }

    // Use streaming
    await sendStreamMessage({
      message,
      session_id: currentSessionId,
      company_id: companyId || undefined,
    })
  } catch (error: any) {
    console.error('Error sending message:', error)
    
    const errorMessage: EnhancedChatMessage = {
      id: Date.now().toString(),
      content: error.message || 'Failed to send message. Please try again.',
      role: 'assistant',
      timestamp: new Date().toISOString(),
      error: true,
    }
    
    setMessages(prev => [...prev, errorMessage])
    setIsLoading(false)
  }
}
```

### Step 4: Update the render section to show streaming status

**FIND THIS PART (around line 160):**
```typescript
<div className="flex-1 overflow-y-auto space-y-6 px-4 md:px-8">
  <AnimatePresence mode="popLayout">
    {messages.map((message) => (
      <EnhancedMessage key={message.id} message={message} />
    ))}
  </AnimatePresence>
  <div ref={messagesEndRef} />
</div>
```

**REPLACE WITH:**
```typescript
<div className="flex-1 overflow-y-auto space-y-6 px-4 md:px-8">
  <AnimatePresence mode="popLayout">
    {messages.map((message) => (
      <EnhancedMessage key={message.id} message={message} />
    ))}
    
    {/* Show streaming status and content */}
    {isStreaming && (
      <motion.div
        initial={{ opacity: 0, y: 20 }}
        animate={{ opacity: 1, y: 0 }}
        className="flex items-start space-x-3"
      >
        <div className="w-8 h-8 rounded-full bg-emerald-500/20 text-emerald-400 flex items-center justify-center flex-shrink-0">
          <Sparkles className="w-5 h-5" />
        </div>
        <div className="flex-1">
          {/* Status bar */}
          {streamStatus && (
            <div className="mb-3 inline-flex items-center px-3 py-1.5 bg-blue-500/10 border border-blue-500/30 rounded-lg text-blue-400 text-sm font-medium">
              <Loader className="w-4 h-4 mr-2 animate-spin" />
              {streamStatus.message}
            </div>
          )}
          
          {/* Streaming content */}
          {currentContent && (
            <div className="rounded-xl p-4 bg-gray-800/50 border border-gray-700/50">
              <div className="prose prose-invert max-w-none">
                <div className="text-white whitespace-pre-wrap">{currentContent}</div>
              </div>
            </div>
          )}
          
          {/* Execution tree while streaming */}
          {currentTree && (
            <div className="mt-3">
              <div className="text-xs text-gray-400 mb-2 font-medium">Execution Progress:</div>
              <div className="bg-gray-900/50 border border-gray-700/30 rounded-lg p-3 space-y-1">
                <div className="text-xs font-medium text-emerald-400">{currentTree.root.name}</div>
                <div className="text-xs text-gray-500">Status: {currentTree.root.status}</div>
                {currentTree.root.duration_ms && (
                  <div className="text-xs text-gray-500">Duration: {currentTree.root.duration_ms}ms</div>
                )}
              </div>
            </div>
          )}
        </div>
      </motion.div>
    )}
  </AnimatePresence>
  <div ref={messagesEndRef} />
</div>
```

## ‚úÖ Test Backend Logging

Run this to verify LLM calls are being logged:

```bash
# Start backend
cd backend
uvicorn app.main:app --reload

# In another terminal, test
curl -X POST http://localhost:8000/api/v1/chat/ \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello", "session_id": "test-123"}'

# Check database
psql your_database -c "SELECT model, total_tokens, created_at FROM llm_calls WHERE session_id = 'test-123' ORDER BY created_at;"
```

You should see LLM calls logged!

## ‚úÖ Test Streaming

```bash
# Test streaming endpoint
curl -N -X POST http://localhost:8000/api/v1/chat/stream \
  -H "Content-Type: application/json" \
  -d '{"message": "What are product gaps?", "company_id": "comp-123"}'

# You'll see real-time events:
# data: {"type":"status","data":{"status":"starting","message":"Initializing..."}}
# data: {"type":"content","data":{"content":"Based"}}
# data: {"type":"content","data":{"content":" on"}}
# ...
```

## About Navigation Issue

The navigation issue (switching pages stops the query) is normal behavior for HTTP requests. When you navigate away, the browser cancels the request. This is standard.

**Solutions:**
1. **Keep streaming running** - The backend continues processing even if frontend disconnects
2. **Backend persistence** - Messages are saved to database even if user navigates away
3. **Resume on return** - When user comes back, they can see the completed response from database

**The stream continues on backend**, it just disconnects from frontend. The completed message is still saved to the database.

## Summary

**Backend:** ‚úÖ FIXED - LLM calls now logging correctly
**Frontend:** ‚ö†Ô∏è Needs manual edit of `chat-view.tsx` following steps above
**Streaming:** ‚úÖ Working on backend, needs frontend integration
**Navigation:** ‚ÑπÔ∏è Normal behavior - backend continues, frontend disconnects

Once you update chat-view.tsx, you'll see:
- üîµ "Initializing..." status
- üîµ "Analyzing query..." status
- ‚ö° Content streaming word-by-word
- üìä Execution tree updating live
- ‚úÖ "Complete" when done

All LLM calls will be in your `llm_calls` database table!

